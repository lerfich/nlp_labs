{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['culture', 'forces', 'travel', 'business', 'style', 'economics', 'sport', 'life', 'science', 'media'] INITIAL LABELS\n",
      "[4, 6, 9, 5, 5, 5, 0, 6, 9, 1, 9, 6, 1, 7, 1, 5, 0, 9, 9, 7, 9, 6, 5, 7, 0, 8, 8, 2, 6, 8, 5, 5, 9, 8, 5, 6, 1, 7, 9, 1, 0, 3, 9, 8, 5, 5, 1, 3, 6, 0, 8, 4, 0, 6, 9, 4, 0, 9, 7, 7, 6, 5, 6, 7, 7, 8, 5, 9, 8, 1, 8, 6, 0, 8, 8, 3, 8, 7, 1, 0, 9, 6, 0, 6, 1, 4, 7, 5, 8, 7, 9, 5, 0, 1, 6, 0, 8, 0, 8, 8, 8, 0, 0, 2, 2, 7, 1, 7, 9, 8, 8, 6, 7, 8, 8, 7, 8, 0, 9, 0, 1, 7, 0, 8, 7, 5, 5, 8, 7, 7, 6, 0, 9, 0, 3, 6, 9, 7, 9, 1, 9, 9, 5, 6, 5, 8, 3, 7, 8, 5, 7, 0, 0, 6, 8, 7, 7, 1, 5, 5, 5, 5, 6, 3, 0, 5, 6, 0, 4, 0, 8, 9, 1, 8, 4, 7, 8, 6, 0, 7, 9, 8, 8, 7, 7, 5, 1, 9, 9, 0, 0, 6, 3, 7, 0, 6, 8, 1, 5, 0, 0, 6, 6, 8, 9, 9, 5, 0, 5, 5, 0, 7, 8, 0, 0, 9, 9, 9, 1, 0, 3, 5, 0, 5, 8, 6, 0, 3, 0, 1, 5, 3, 5, 7, 9, 2, 7, 6, 5, 9, 3, 5, 8, 7, 7, 1, 5, 1, 8, 0, 0, 8, 9, 1, 5, 5, 7, 0, 9, 7, 0, 6, 6, 1, 7, 0, 6, 6, 8, 7, 8, 9, 9, 7, 7, 9, 0, 0, 9, 3, 9, 5, 9, 8, 6, 5, 7, 0, 6, 0, 5, 6, 9, 9, 1, 8, 1, 7, 6, 7, 5, 0, 8, 0, 0, 6, 5, 7, 6, 6, 8, 7, 6, 8, 1, 6, 0, 7, 7, 8, 9, 6, 6, 8, 6, 0, 8, 9, 0, 5, 5, 7, 6, 5, 9, 8, 1, 8, 0, 0, 1, 7, 8, 9, 6, 9, 8, 9, 6, 6, 8, 3, 5, 0, 5, 8, 8, 6, 6, 2, 9, 9, 1, 6, 7, 0, 8, 6, 6, 8, 0, 9, 6, 4, 0, 8, 5, 0, 8, 1, 8, 8, 7, 5, 8, 9, 5, 7, 3, 9, 5, 6, 3, 5, 9, 5, 8, 6, 8, 5, 6, 8, 0, 8, 0, 0, 6, 9, 9, 5, 1, 1, 0, 4, 0, 9, 2, 8, 8, 7, 6, 5, 6, 5, 7, 9, 7, 0, 6, 8, 1, 3, 8, 7, 6, 9, 0, 6, 9, 4, 5, 6, 1, 1, 0, 8, 0, 6, 8, 0, 6, 0, 4, 2, 5, 6, 0, 1, 7, 7, 8, 5, 0, 0, 7, 0, 7, 6, 9, 6, 9, 7, 8, 5, 9, 6, 9, 8, 6, 8, 5, 8, 8, 9, 8, 6, 5, 8, 0, 7, 0, 5, 0, 6, 8, 0, 8, 7, 8, 1] NUMERICAL LABELS\n",
      "[6 8 0 8 0 0 6 1 9 5 8 2 9 4 0 6 7 7 7 7 6 5 6 5 6 7 7 2 6 8 1 0 9 0 6 9 0\n",
      " 0 9 4 5 6 1 0 0 8 0 2 1 0 1 5 4 2 5 6 0 1 7 7 8 5 0 0 7 0 7 6 9 6 3 7 8 5\n",
      " 9 6 2 8 6 8 3 8 8 7 9 6 5 1 0 0 4 5 0 6 8 0 8 7 8 7]\n",
      "Accuracy: 0.71\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# import nltk\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from sklearn import svm, metrics\n",
    "\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stop_words=set(stopwords.words('russian'))\n",
    "\n",
    "\n",
    "\n",
    "file = open(\"news.txt\")\n",
    "full_text = ''\n",
    "texts = []\n",
    "labels = []\n",
    "sentences = []\n",
    "\n",
    "\n",
    "for item in file.readlines()[0:500]:\n",
    "    # print(index)\n",
    "    extracted_label = item.split('\\t')[0]\n",
    "    splitted_news_text = item.split('\\t')[2]\n",
    "    full_text = full_text + splitted_news_text\n",
    "    \n",
    "    cleared_text = ''\n",
    "    for word in splitted_news_text.split(' '):\n",
    "        lemmatizied_word = wordnet_lemmatizer.lemmatize(word)\n",
    "        if (lemmatizied_word not in stop_words):\n",
    "            cleared_text = cleared_text + word + ' '\n",
    "\n",
    "\n",
    "    labels.append(extracted_label)\n",
    "    texts.append(cleared_text)\n",
    "\n",
    "\n",
    "\n",
    "int_labels = []\n",
    "labels_set = list(set(labels))\n",
    "for label in labels:\n",
    "   for index, unique_label in enumerate(labels_set):\n",
    "       if (label == unique_label):\n",
    "           int_labels.append(index)\n",
    "           break\n",
    "\n",
    "print(labels_set, 'INITIAL LABELS')              \n",
    "print(int_labels, 'NUMERICAL LABELS')       \n",
    "\n",
    "data = pd.DataFrame(list(zip(texts))) \n",
    "data.columns = ['response']\n",
    "\n",
    "response_new = data.response.apply(simple_preprocess)\n",
    "# print(response_new)\n",
    "\n",
    "# Model parameters\n",
    "model=Word2Vec(window=25, min_count=2, workers=4, sg=1)\n",
    "\n",
    "# Train the model\n",
    "model.build_vocab(response_new, progress_per=1000)\n",
    "model.train(response_new, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "\n",
    "# corpus_mean_vectors = []\n",
    "# for corpus in response_new:\n",
    "#     corpus_mean_vectors.append(model.wv.get_mean_vector(keys=corpus))\n",
    "\n",
    "\n",
    "corpus_mean_vectors = []\n",
    "for corpus in response_new:\n",
    "    accumulative_vector = np.zeros(100)\n",
    "    for word in corpus:\n",
    "        word_distances = []\n",
    "        best_vector = word\n",
    "        # for wordJ in corpus:\n",
    "        #     if (word in model.wv):\n",
    "        #         word_vector = model.wv[word]\n",
    "        #         # word_distances.append(model.wv.distance(word, wordJ))\n",
    "        #     else:\n",
    "        #         word_vector = np.zeros(100)\n",
    "\n",
    "        if (word in model.wv):\n",
    "            word_vector = model.wv[word]\n",
    "        else:\n",
    "            word_vector = np.zeros(100)\n",
    "        for index, coord in enumerate(word_vector):\n",
    "            accumulative_vector[index] = accumulative_vector[index] - coord #0.68\n",
    "            # accumulative_vector[index] = accumulative_vector[index] + coord # 0.7\n",
    "            # accumulative_vector[index] = min(accumulative_vector[index], coord) # 0.5\n",
    "            # accumulative_vector[index] = max(accumulative_vector[index], coord) # 0.45\n",
    "            # accumulative_vector[index] = accumulative_vector[index] + coord #0.56\n",
    "\n",
    "    # for index, coord in enumerate(accumulative_vector):\n",
    "    #     accumulative_vector[index] = coord / len(corpus)\n",
    "\n",
    "    corpus_mean_vectors.append(accumulative_vector)\n",
    "\n",
    "\n",
    "train_samples_size = ceil(len(corpus_mean_vectors) * 0.8)\n",
    "X_train = corpus_mean_vectors[0: train_samples_size]\n",
    "X_test = corpus_mean_vectors[train_samples_size:]\n",
    "Y_train = int_labels[0: train_samples_size]\n",
    "Y_test = int_labels[train_samples_size:]\n",
    "\n",
    "\n",
    "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "clf.fit(X_train, Y_train)\n",
    "Y_pred = clf.predict(X_test)\n",
    "print(Y_pred)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred))\n",
    "# Model Precision: what percentage of positive tuples are labeled as such?\n",
    "# print(\"Precision:\",metrics.precision_score(Y_test, Y_pred, average=\"weighted\"))\n",
    "# # Model Recall: what percentage of positive tuples are labelled as such?\n",
    "# print(\"Recall:\",metrics.recall_score(Y_test, Y_pred, average=\"weighted\"))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
